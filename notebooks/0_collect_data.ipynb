{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep, time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all the text bodys from the soup\n",
    "def get_text_bodys_from_soup(soup):\n",
    "    return soup.find_all('tbody', \n",
    "                        # x and x.startswith('normalthread_') means that \n",
    "                        # x is not None and starts with 'normalthread_' \n",
    "                        attrs={'id': lambda x: x and \n",
    "                                        x.startswith('normalthread_') or\n",
    "                                        x.startswith('stickthread_')})\n",
    "\n",
    "# Function to get all the comments from the text bodys\n",
    "def get_comments_from_text_bodys(text_bodys):\n",
    "    return [text_body.find('a', attrs={'class': 's xst'}).text \n",
    "                    for text_body in text_bodys]\n",
    "\n",
    "# Function to get all the summaries from the text bodys\n",
    "def get_summaries_from_text_bodys(text_bodys):\n",
    "    return [text_body.find('a', attrs={'href': lambda x: x and \n",
    "                                            x.startswith('forum.php?')}).text\n",
    "                for text_body in text_bodys]\n",
    "\n",
    "# Function to get all the comments and summaries from the soup\n",
    "def get_comments_from_soup(soup):\n",
    "    text_bodys = get_text_bodys_from_soup(soup)\n",
    "    comments = get_comments_from_text_bodys(text_bodys)\n",
    "    summaries = get_summaries_from_text_bodys(text_bodys)\n",
    "    if len(comments) != len(summaries):\n",
    "        print('len(comments) != len(summaries)')\n",
    "        return None\n",
    "    return comments, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all the comments and summaries from the url\n",
    "def get_comments_summaries_from_page(page_url, page_num):\n",
    "    response = requests.get(page_url)\n",
    "    if response.status_code != 200:\n",
    "        print('page num: ', page_num, ', status code: ', response.status_code)\n",
    "        print('page_url: ', page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return get_comments_from_soup(soup)\n",
    "\n",
    "# Function to get all the comments and summaries from all the pages\n",
    "def get_data_from_pages(base_url, start_page=1, end_page=1000):\n",
    "    all_comments = []\n",
    "    all_summaries = []\n",
    "    for page_num in range(start_page, end_page+1):\n",
    "        page_url = base_url + str(page_num) + '.html'\n",
    "        comments, summaries = get_comments_summaries_from_page(page_url, \n",
    "                                                                page_num)\n",
    "        if comments is None:\n",
    "            break\n",
    "        all_comments.extend(comments)\n",
    "        all_summaries.extend(summaries)\n",
    "        # sleep for 3-10 seconds randomly to avoid being blocked\n",
    "        sleep(np.random.randint(3, 10))\n",
    "    return pd.DataFrame({'comment': all_comments, 'summary': all_summaries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  9.689262866973877\n",
      "Time elapsed:  0:00:09.689263\n",
      "Content saved to comment_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to run the main function\n",
    "def main():\n",
    "    # time the running time of the function\n",
    "    start_time = time()\n",
    "    base_url = 'fake_urlxxxxxxx.com' # This is a fake url\n",
    "    df = get_data_from_pages(base_url, start_page=1, end_page=2)\n",
    "    end_time = time()\n",
    "    print('Time elapsed: ', end_time - start_time)\n",
    "    print('Time elapsed: ', str(datetime.timedelta(seconds=end_time-start_time)))\n",
    "    # save the content to a csv file, overwrite if the file already exists\n",
    "    df.to_csv('comment_summary.csv', index=False)\n",
    "    print('Content saved to comment_summary.csv')\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
